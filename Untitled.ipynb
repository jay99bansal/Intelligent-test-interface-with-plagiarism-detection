{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = 'Plagiarism detection is the process of locating instances of plagiarism within a work or document. The widespread use of computers and the advent of the Internet have made it easier to plagiarize the work of others. Detection of plagiarism can be undertaken in a variety of ways. Human detection is the most traditional form of identifying plagiarism from written work. This can be a lengthy and time-consuming task for the reader and can also result in inconsistencies in how plagiarism is identified within an organization. Text-matching software (TMS), which is also referred to as \"plagiarism detection software\" or \"anti-plagiarism\" software, has become widely available, in the form of both commercially available products as well as open-source software. TMS does not actually detect plagiarism per se, but instead finds specific passages of text in one document that match text in another document.'\n",
    "\n",
    "s2 = 'Plagiarism detection is defined the process of locating instances of plagiarism within a document. TMS does not actually detect plagiarism exactly, but it finds specific passages of text in one document that match text in another document. Traditional form of identifying plagiarism from written work is human detection.'\n",
    "\n",
    "s3 = 'To detect plagiarism of any form, it is essential to have broad knowledge of its possible forms and classes, and existence of various tools and systems for its detection. Based on impact or severity of damages, plagiarism may occur in an article or in any production in a number of ways. This survey presents a taxonomy of various plagiarism forms and include discussion on each of these forms. Over the years, a good number tools and techniques have been introduced to detect plagiarism. This paper highlights few promising methods for plagiarism detection based on machine learning techniques. We analyse the pros and cons of these methods and finally we highlight a list of issues and research challenges related to this evolving research problem.'\n",
    "\n",
    "s4 = 'Plagiarism detection'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7284768211920529\n",
      "0.7493333333333333\n",
      "0.7141280353200883\n",
      "0.977924944812362\n",
      "0.9375\n",
      "0.9746666666666667\n"
     ]
    }
   ],
   "source": [
    "from similarity.normalized_levenshtein import NormalizedLevenshtein\n",
    "normalized_levenshtein = NormalizedLevenshtein()\n",
    "print(normalized_levenshtein.distance(s1,s2))\n",
    "print(normalized_levenshtein.distance(s2,s3))\n",
    "print(normalized_levenshtein.distance(s3,s1))\n",
    "print(normalized_levenshtein.distance(s4,s1))\n",
    "print(normalized_levenshtein.distance(s4,s2))\n",
    "print(normalized_levenshtein.distance(s4,s3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2814569536423841\n",
      "0.2666666666666666\n",
      "0.41501103752759383\n",
      "0.02207505518763797\n",
      "0.0625\n",
      "0.02533333333333332\n"
     ]
    }
   ],
   "source": [
    "from similarity.longest_common_subsequence import LongestCommonSubsequence\n",
    "from similarity.metric_lcs import MetricLCS\n",
    "\n",
    "def met_lcs(s1,s2):\n",
    "    lcs = LongestCommonSubsequence()\n",
    "    metric_lcs = MetricLCS()\n",
    "    dist = lcs.distance(s1,s2)\n",
    "    # our metric\n",
    "    #print((len(s1)+len(s2)-dist)/(2*len(s1)))\n",
    "    return 1-metric_lcs.distance(s1, s2)\n",
    "print(met_lcs(s1,s2))\n",
    "print(met_lcs(s2,s3))\n",
    "print(met_lcs(s3,s1))\n",
    "print(met_lcs(s1,s4))\n",
    "print(met_lcs(s2,s4))\n",
    "print(met_lcs(s3,s4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4082482904638631\n",
      "0.4082482904638631\n",
      "1.0\n",
      "0.4082482904638631\n",
      "1.0000000000000002\n",
      "0.4082482904638631\n"
     ]
    }
   ],
   "source": [
    "from similarity.cosine import Cosine\n",
    "\n",
    "def met_cosine(s1,s2,n):\n",
    "    cosine = Cosine(n)\n",
    "    p1 = cosine.get_profile(s1)\n",
    "    p2 = cosine.get_profile(s2)\n",
    "    return cosine.similarity_profiles(p1, p2)\n",
    "    \n",
    "n = 2\n",
    "print(met_cosine(s1,s2,n))\n",
    "print(met_cosine(s2,s3,n))\n",
    "print(met_cosine(s3,s1,n))\n",
    "print(met_cosine(s1,s4,n))\n",
    "print(met_cosine(s2,s4,n))\n",
    "print(met_cosine(s3,s4,n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.875\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.2\n"
     ]
    }
   ],
   "source": [
    "from similarity.jaccard import Jaccard\n",
    "\n",
    "def met_jaccard(s1,s2,n):\n",
    "    jac = Jaccard(n)\n",
    "    return jac.similarity(s1, s2)\n",
    "    \n",
    "n = 3\n",
    "print(met_jaccard(s1,s2,n))\n",
    "print(met_jaccard(s2,s3,n))\n",
    "print(met_jaccard(s3,s1,n))\n",
    "print(met_jaccard(s1,s4,n))\n",
    "print(met_jaccard(s2,s4,n))\n",
    "print(met_jaccard(s3,s4,n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6049519752084935\n",
      "0.3426514449414421\n",
      "0.40761883623746475\n",
      "0.35606211812585503\n",
      "0.3782826230835536\n",
      "0.29904896777007306\n"
     ]
    }
   ],
   "source": [
    "def met_weighted(str1, str2):\n",
    "    ind = []\n",
    "    #ind.append(met_lcs(str1, str2))\n",
    "    val1 = 0\n",
    "    val2 = 0\n",
    "    val3 = 0\n",
    "    val4 = 0\n",
    "    p = 1.2\n",
    "    for i in range(1,5):\n",
    "        k = i**p\n",
    "        val4 += k\n",
    "        val1 += met_cosine(str1, str2,i)*k\n",
    "        val2 += met_jaccard(str1, str2,i)*k\n",
    "        val3 += met_cosine_word(str1, str2,i)*k\n",
    "    val1 = val1/val4\n",
    "    val2 = val2/val4\n",
    "    val3 = val3/val4\n",
    "    # ind.append(met_lcs(str1, str2))\n",
    "    ind.append(val1)\n",
    "    ind.append(val2)\n",
    "    ind.append(val3)\n",
    "    met_weights = [0.5, 0.5, 0.0]\n",
    "    ans = 0\n",
    "    for i in range(0,len(ind)):\n",
    "        ans+=ind[i]*met_weights[i]\n",
    "    return ans\n",
    "\n",
    "\n",
    "s1 = pre_process(s1)\n",
    "s2 = pre_process(s2)\n",
    "s3 = pre_process(s3)\n",
    "s4 = pre_process(s4)\n",
    "print(met_weighted(s1,s2))\n",
    "print(met_weighted(s2,s3))\n",
    "print(met_weighted(s3,s1))\n",
    "print(met_weighted(s1,s4))\n",
    "print(met_weighted(s2,s4))\n",
    "print(met_weighted(s3,s4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3487772492870674\n",
      "0.0\n",
      "0.006950816264905735\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "import math\n",
    "\n",
    "from nltk import word_tokenize \n",
    "from nltk.util import ngrams\n",
    "\n",
    "def dotproduct(v1, v2):\n",
    "    return sum((a*b) for a, b in zip(v1, v2))\n",
    "\n",
    "def length(v):\n",
    "    return math.sqrt(dotproduct(v, v))\n",
    "\n",
    "def cosangle(v1, v2):\n",
    "    if ((length(v1) * length(v2)) != 0):\n",
    "        return dotproduct(v1, v2) / (length(v1) * length(v2))\n",
    "    return 0\n",
    "\n",
    "def met_cosine_word(s1,s2,n):\n",
    "    token = word_tokenize(s1)\n",
    "    ngram = list(ngrams(token, n))\n",
    "    ngram = ['~'.join(i) for i in ngram]\n",
    "    freq1 = {} \n",
    "    for item in ngram: \n",
    "        if (item in freq1): \n",
    "            freq1[item] += 1\n",
    "        else: \n",
    "            freq1[item] = 1\n",
    "    token = word_tokenize(s2)\n",
    "    ngram = list(ngrams(token, n))\n",
    "    ngram = ['~'.join(i) for i in ngram]\n",
    "    freq2 = {} \n",
    "    for item in ngram: \n",
    "        if (item in freq2): \n",
    "            freq2[item] += 1\n",
    "        else: \n",
    "            freq2[item] = 1\n",
    "    alldict = [freq1, freq2]\n",
    "    allkey = functools.reduce(set.union, map(set, map(dict.keys, alldict)))\n",
    "    vec1 = []\n",
    "    vec2 = []\n",
    "    for key in allkey:\n",
    "        if key in freq1:\n",
    "            vec1.append(freq1[key])\n",
    "        else:\n",
    "            vec1.append(0)\n",
    "        if key in freq2:\n",
    "            vec2.append(freq2[key])\n",
    "        else:\n",
    "            vec2.append(0)\n",
    "    return cosangle(vec1, vec2)\n",
    "\n",
    "n = 3\n",
    "print(met_cosine_word(s1,s2,n))\n",
    "print(met_cosine_word(s2,s3,n))\n",
    "print(met_cosine_word(s3,s1,n))\n",
    "print(met_cosine_word(s1,s4,n))\n",
    "print(met_cosine_word(s2,s4,n))\n",
    "print(met_cosine_word(s3,s4,n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def pre_process(s):\n",
    "    s = s.translate(str.maketrans('', '', string.punctuation))\n",
    "    #s = s.replace(\" \", \"\")\n",
    "    return s.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = 'hey there, I am using whatsapp!'\n",
    "s2 = 'hey there; i am using whatsapp'\n",
    "s3 = 'h a l l l'\n",
    "s4 = 'h e l l l'\n",
    "s1 = pre_process(s1)\n",
    "s2 = pre_process(s2)\n",
    "s1 = pre_process(s1)\n",
    "s2 = pre_process(s2)\n",
    "s3 = pre_process(s3)\n",
    "s4 = pre_process(s4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
